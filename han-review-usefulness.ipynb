{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Understanding review usefulness through attention using Hierarchical Attention Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOugyDAcfygu",
        "colab_type": "text"
      },
      "source": [
        "## Understanding review usefulness through attention using Hierarchical Attention Network (HAN)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E4fHE5EL769M",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import json\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from fastai.text import Tokenizer, Vocab, SpacyTokenizer, WeightDropout, EmbeddingDropout\n",
        "from gensim.utils import tokenize\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from textblob import TextBlob\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model_name = 'amazon_electronics_useful'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oYbhlP-D_HM",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will be using a **subset of the Amazon Electronics 5-core review dataset**, and GloVe 300D word embeddings.\n",
        "\n",
        "Original dataset is obtained from https://nijianmo.github.io/amazon/index.html. Reviews from other Amazon categories can also be obtained from the same link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QzsSEyI_cIL",
        "colab_type": "code",
        "outputId": "47380805-d9c7-440c-aba9-540c92c6b542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "path = '/tmp/'\n",
        "\n",
        "# Glove 300D embeddings\n",
        "!gdown --id '1Bn1s3w4eETF3yDHJmhLXNOPSN7q29sey' --output '/tmp/glove.6B.300d.txt'\n",
        "\n",
        "# Amazon Electronics dataset\n",
        "!gdown --id '1JE38UqpHNFycbi3B6gOu4OksjVhck8bP' --output '/tmp/Electronics_5.csv'\n",
        "dataset_filename = 'Electronics_5.csv'\n",
        "df = pd.read_csv(path + dataset_filename)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Bn1s3w4eETF3yDHJmhLXNOPSN7q29sey\n",
            "To: /tmp/glove.6B.300d.txt\n",
            "1.04GB [00:13, 75.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JE38UqpHNFycbi3B6gOu4OksjVhck8bP\n",
            "To: /tmp/Electronics_5.csv\n",
            "548MB [00:12, 42.4MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWWHu9Z3-r3P",
        "outputId": "410e5549-0500-42b6-db2f-d500c27323ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df['reviewText'] = df['reviewText'].astype(str)\n",
        "df['vote'] = df['vote'].str.replace(',','').fillna(0)\n",
        "df['overall'] = df['overall'].fillna(0)\n",
        "df['vote'] = pd.to_numeric(df['vote'])\n",
        "df['overall'] = pd.to_numeric(df['overall'])\n",
        "df = df.filter(['vote','overall','reviewText', 'reviewTime'], axis=1)\n",
        "df.columns = ['useful', 'stars', 'text', 'date']\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>useful</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67</td>\n",
              "      <td>5.0</td>\n",
              "      <td>This is the best novel I have read in 2 or 3 y...</td>\n",
              "      <td>09 18, 1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Pages and pages of introspection, in the style...</td>\n",
              "      <td>10 23, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>This is the kind of novel to read when you hav...</td>\n",
              "      <td>09 2, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>5.0</td>\n",
              "      <td>What gorgeous language! What an incredible wri...</td>\n",
              "      <td>09 4, 2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I was taken in by reviews that compared this b...</td>\n",
              "      <td>02 4, 2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>The Netgear Router is my first experience with...</td>\n",
              "      <td>06 19, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>This router targets people who want custom fir...</td>\n",
              "      <td>06 18, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>This router is great for what it's advertised ...</td>\n",
              "      <td>06 13, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I popped the CD into my PC and followed the in...</td>\n",
              "      <td>05 31, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>This wireless -g router appears to be adequate...</td>\n",
              "      <td>05 29, 2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        useful  ...         date\n",
              "0           67  ...  09 18, 1999\n",
              "1            5  ...  10 23, 2013\n",
              "2            4  ...   09 2, 2008\n",
              "3           13  ...   09 4, 2000\n",
              "4            8  ...   02 4, 2000\n",
              "...        ...  ...          ...\n",
              "999995       2  ...  06 19, 2008\n",
              "999996       2  ...  06 18, 2008\n",
              "999997       3  ...  06 13, 2008\n",
              "999998       3  ...  05 31, 2008\n",
              "999999       5  ...  05 29, 2008\n",
              "\n",
              "[1000000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "frneoR33tdEl"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oj64hlMF_H0Z",
        "outputId": "2f5c6e7a-46dc-451d-f975-03f5f321b10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "# Plot distributions of useful votes, text length and review scores\n",
        "plt.figure(figsize=(12,3))\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,3))\n",
        "ax1.hist(df['useful'], range=(0, 100), bins=100);\n",
        "ax2.hist(df['text'].str.len(), bins=100);\n",
        "ax3.hist(df['stars'], range=(0.5, 5.5), bins=5);\n",
        "ax1.title.set_text('Useful Votes');\n",
        "ax2.title.set_text('Text Length (by char)');\n",
        "ax3.title.set_text('Review Scores');"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x216 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4S8t9hEJuaak",
        "outputId": "7a630a32-efcd-413f-ab9c-a29019aeec0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Count of reviews by useful votes\n",
        "df[\"useful\"].value_counts()[:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     817046\n",
              "2      49766\n",
              "3      28965\n",
              "4      18299\n",
              "5      13018\n",
              "6       9615\n",
              "7       7438\n",
              "8       5832\n",
              "9       4889\n",
              "10      4016\n",
              "11      3410\n",
              "12      2892\n",
              "13      2542\n",
              "14      2188\n",
              "15      1991\n",
              "16      1751\n",
              "17      1538\n",
              "18      1390\n",
              "19      1241\n",
              "20      1117\n",
              "Name: useful, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "atzOMCvQDle8",
        "outputId": "8a82f39b-604c-43be-8abc-9cff144b9b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Oldest review: {}\".format(df['date'].min()))\n",
        "print(\"Latest review: {}\".format(df['date'].max()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oldest review: 01 1, 2000\n",
            "Latest review: 12 9, 2017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLn_hPcyuuTV"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmg4EEMv_AV4",
        "colab_type": "text"
      },
      "source": [
        "We treat the prediction of review usefulness as a binary classification task. Here, we define a useful review as one with >=9 useful votes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uBLNlZ_mBRyn",
        "outputId": "cd7dbb48-b2d7-4778-91cc-4b2a79746c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Threshold for deciding whether a review is useful or not\n",
        "threshold = 9\n",
        "useful_count = len(df[df['useful'] >= threshold]) # Count of reviews with >= X votes\n",
        "useful_count"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50021"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6THoZJBeEWhv",
        "outputId": "9a6c227d-d6d4-4b09-e1e9-03d564d2c501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Construct balanced dataset by downsampling non-useful reviews\n",
        "# Can consider upsampling useful reviews instead if we have enough capacity to train lots of data on Colab\n",
        "balanced_df = df[df['useful'] >= threshold].sample(50000, random_state = seed)\n",
        "balanced_df = pd.concat([balanced_df, df[df['useful'] < threshold].sample(50000, random_state = seed)])\n",
        "balanced_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>useful</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>181986</th>\n",
              "      <td>9</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Whoa!  What a neat package.  Simple mic with j...</td>\n",
              "      <td>07 15, 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784109</th>\n",
              "      <td>17</td>\n",
              "      <td>5.0</td>\n",
              "      <td>This is a great camcorder. I use the highest q...</td>\n",
              "      <td>06 18, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130169</th>\n",
              "      <td>29</td>\n",
              "      <td>5.0</td>\n",
              "      <td>After extensive research, I decided on this ca...</td>\n",
              "      <td>03 28, 2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388072</th>\n",
              "      <td>15</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Ordered this Tiffin filter as part of the purc...</td>\n",
              "      <td>01 21, 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812642</th>\n",
              "      <td>14</td>\n",
              "      <td>2.0</td>\n",
              "      <td>This would be a great network appliance if I w...</td>\n",
              "      <td>04 23, 2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109878</th>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>A portable device with more bulk than expected...</td>\n",
              "      <td>11 30, 2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662412</th>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Thanx</td>\n",
              "      <td>12 27, 2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Great mount used it on my \"32 TV and not I can...</td>\n",
              "      <td>07 27, 2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763804</th>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>The memory card reader works great and is easy...</td>\n",
              "      <td>10 3, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639241</th>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>I used the free trial of Network Magic to set ...</td>\n",
              "      <td>04 2, 2009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        useful  ...         date\n",
              "181986       9  ...  07 15, 2011\n",
              "784109      17  ...  06 18, 2008\n",
              "130169      29  ...  03 28, 2003\n",
              "388072      15  ...  01 21, 2011\n",
              "812642      14  ...  04 23, 2008\n",
              "...        ...  ...          ...\n",
              "109878       0  ...  11 30, 2007\n",
              "662412       0  ...  12 27, 2014\n",
              "404          0  ...  07 27, 2015\n",
              "763804       0  ...   10 3, 2013\n",
              "639241       0  ...   04 2, 2009\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CgDROAxJE8z5",
        "colab": {}
      },
      "source": [
        "# Split into train/dev/test at 80%/10%/10%\n",
        "df2 = balanced_df.drop(labels=[\"date\"], axis=1)\n",
        "df2 = df2.reindex(columns=[\"text\", \"useful\", \"stars\"])\n",
        "df2['useful'] = np.where(df2['useful'] >= threshold, 1, 0) # Convert 'useful' into binary labels\n",
        "df2['stars'] = df2['stars'].astype('long')\n",
        "\n",
        "train, val, test = np.split(df2.sample(frac=1, random_state=seed), [int(.8*len(df2)), int(.9*len(df2))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qSq4RT6I_f6t",
        "outputId": "dea21545-ba35-4afb-8d17-3f466a208f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>597703</th>\n",
              "      <td>Let me start by saying that I am picky and a b...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862033</th>\n",
              "      <td>Just wanted a non-cable TV for kitchen.  Doesn...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823996</th>\n",
              "      <td>I purchased this to connect a pair of headphon...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255531</th>\n",
              "      <td>I purchased the MDX1600 to work on improving t...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665792</th>\n",
              "      <td>If you order a lot of CDs from Amazon, then we...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621925</th>\n",
              "      <td>haven't used it yet its in my earth quake disa...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8633</th>\n",
              "      <td>Bought this for a spare just in case.....It lo...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529362</th>\n",
              "      <td>It is ok, but it is slap when it us connected.</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360247</th>\n",
              "      <td>Excellent radio and reception in this mostly r...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497998</th>\n",
              "      <td>I am writing a review regarding the SONY KDL-4...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  useful  stars\n",
              "597703  Let me start by saying that I am picky and a b...       1      3\n",
              "862033  Just wanted a non-cable TV for kitchen.  Doesn...       0      3\n",
              "823996  I purchased this to connect a pair of headphon...       0      5\n",
              "255531  I purchased the MDX1600 to work on improving t...       1      5\n",
              "665792  If you order a lot of CDs from Amazon, then we...       1      5\n",
              "...                                                   ...     ...    ...\n",
              "621925  haven't used it yet its in my earth quake disa...       0      4\n",
              "8633    Bought this for a spare just in case.....It lo...       0      5\n",
              "529362     It is ok, but it is slap when it us connected.       0      3\n",
              "360247  Excellent radio and reception in this mostly r...       1      5\n",
              "497998  I am writing a review regarding the SONY KDL-4...       1      5\n",
              "\n",
              "[80000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWV82X4BwLxw",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**TARGET**</font>\n",
        "\n",
        "Here, we define the target with which the model will train on. To train the model on review sentiment, set target to 'stars'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtnJyDrawI5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = 'useful'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q8iOC05rXXyW"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4vI8Z98NSHR",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**SPLIT_SENTENCES**</font>\n",
        "\n",
        "Since a document can be viewed as having a hierarchical structure where words forms sentences, and sentences form the document, we will require the sentences to be split. We set a flag here so we can choose to treat the entire document as a single data point for other attention models which treats a document as a single entity of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8xbqJwao2Nl7",
        "colab": {}
      },
      "source": [
        "# Whether to split sentences or not (True for HAN, False for other attention models)\n",
        "split_sentences = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qjX_cz4ubAXP",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
        "\n",
        "# Split each review into sentences\n",
        "def generate_texts_sents(dataset):\n",
        "  texts_sents = []\n",
        "  for doc in dataset:\n",
        "      texts_sents.append(tokenizer.tokenize(doc))\n",
        "  return texts_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-hUuGeYyPNt",
        "outputId": "bb206262-1f7d-41ca-e28c-916d5cc91ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if split_sentences:\n",
        "    # Split each review into sentences\n",
        "    train_texts_sents = generate_texts_sents(train.text.tolist())\n",
        "    val_texts_sents = generate_texts_sents(val.text.tolist())\n",
        "    test_texts_sents = generate_texts_sents(test.text.tolist())\n",
        "\n",
        "    # Flatten all sentences in reviews into 1 long list of sentences\n",
        "    train_all_sents = [s for sents in train_texts_sents for s in sents]\n",
        "    val_all_sents = [s for sents in val_texts_sents for s in sents]\n",
        "    test_all_sents = [s for sents in test_texts_sents for s in sents]\n",
        "\n",
        "    # Save the lengths of the documents: 1) for padding purposes and 2) to compute consecutive ranges \n",
        "    # so we can \"fold\" the list again\n",
        "    train_texts_length = [0] + [len(s) for s in train_texts_sents]\n",
        "    val_texts_length = [0] + [len(s) for s in val_texts_sents]\n",
        "    test_texts_length = [0] + [len(s) for s in test_texts_sents]\n",
        "\n",
        "    train_range_idx = [sum(train_texts_length[: i + 1]) for i in range(len(train_texts_length))]\n",
        "    val_range_idx = [sum(val_texts_length[: i + 1]) for i in range(len(val_texts_length))]\n",
        "    test_range_idx = [sum(test_texts_length[: i + 1]) for i in range(len(test_texts_length))]\n",
        "\n",
        "    print(val_texts_sents[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['At the suggested retail price, I would never have made this purchase, but because it was a reasonable Gold Box offer, I leapt toward my first bluetooth earpiece.', \"The pink was not a selling point, but I preferred that to a more common black contraption, and I'm not at all disappointed.\", \"It's more of a mauve than pink which was more pleasing to me.\", 'It synched up immediately with my cell phone and I suspect it will do the same when this phone is replaced in 2009.', 'The manual was easy to read and understand and I had no difficulty placing the earpiece into the lovely leather carrying case/charger or removing it to use.', 'Although some have complained about short battery time, I have not experienced that and have not have the battery die at any time.', 'It is easy to check the battery level by depressing the two buttons on the device and observing the number of red flashes for remaining power.', \"The small size might be easy to lose for some, but I'm aware of that possibility and exercise caution when I put it in my bag or pocket.\", 'Because of that small size, it takes only a second to plug the earpiece into the ear and I do not wear it unless I am on a call.', \"Realizing that it's lightweight, I am also cautious in more windy situations.\", 'I had only one experience with a less than perfect call and was a bit disturbed until I realized the static was generated from the other end of the call.', \"In general, I like the fit, clarity, ease of use, and don't feel like I have a growth erupting from my ear.\", \"I'd buy this again.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYvUifhubbur",
        "colab": {}
      },
      "source": [
        "# Convert to lowercase / Remove accent marks / Limit min & max length of token\n",
        "def simple_preprocess(doc, lower=True, deacc=False, min_len=2, max_len=15):\n",
        "    tokens = [\n",
        "        token\n",
        "        for token in tokenize(doc, lower=lower, deacc=deacc, errors=\"ignore\")\n",
        "        # if min_len <= len(token) <= max_len and not token.startswith(\"_\")\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "# Generate word tokens for review\n",
        "def tokenize_review(texts, with_preprocess=True):\n",
        "    if with_preprocess:\n",
        "        texts = [\" \".join(simple_preprocess(s)) for s in texts]\n",
        "    tokenizer = Tokenizer()\n",
        "    tok = SpacyTokenizer('en')\n",
        "    tokens = []\n",
        "    for text in texts:\n",
        "      tokens.append(tokenizer.process_text(text, tok))\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rdCde4p-b3P4",
        "outputId": "63d7e8c8-9685-4687-dd96-7f0311497a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if split_sentences:\n",
        "  train_sents_tokens = tokenize_review(train_all_sents)\n",
        "  val_sents_tokens = tokenize_review(val_all_sents)\n",
        "  test_sents_tokens = tokenize_review(test_all_sents)\n",
        "  print(val_sents_tokens[0])\n",
        "else: \n",
        "  train_tokens = tokenize_review(train.text.tolist())\n",
        "  val_tokens = tokenize_review(val.text.tolist())\n",
        "  test_tokens = tokenize_review(test.text.tolist())\n",
        "  print(val_tokens[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['at', 'the', 'suggested', 'retail', 'price', 'i', 'would', 'never', 'have', 'made', 'this', 'purchase', 'but', 'because', 'it', 'was', 'a', 'reasonable', 'gold', 'box', 'offer', 'i', 'leapt', 'toward', 'my', 'first', 'bluetooth', 'earpiece']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oB34NRBrcSqk",
        "colab": {}
      },
      "source": [
        "# Create Vocabulary using fastai's Vocab class\n",
        "vocab = Vocab.create(train_sents_tokens if split_sentences else train_tokens, max_vocab=10000, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uJFElyULcWp9",
        "outputId": "fd189173-99f8-43d4-d8b1-77ba27d109fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if split_sentences:\n",
        "  # 'numericalize' each sentence\n",
        "  train_sents_numz = [vocab.numericalize(s) for s in train_sents_tokens]\n",
        "  val_sents_numz = [vocab.numericalize(s) for s in val_sents_tokens]\n",
        "  test_sents_numz = [vocab.numericalize(s) for s in test_sents_tokens]\n",
        "\n",
        "  # group the sentences again into documents\n",
        "  train_texts_numz = [train_sents_numz[train_range_idx[i] : train_range_idx[i + 1]] for i in range(len(train_range_idx[:-1]))]\n",
        "  val_texts_numz = [val_sents_numz[val_range_idx[i] : val_range_idx[i + 1]] for i in range(len(val_range_idx[:-1]))]\n",
        "  test_texts_numz = [test_sents_numz[test_range_idx[i] : test_range_idx[i + 1]] for i in range(len(test_range_idx[:-1]))]\n",
        "\n",
        "  print(val_texts_numz[0])\n",
        "else:\n",
        "  train_texts_numz = [vocab.numericalize(s) for s in train_tokens]\n",
        "  val_texts_numz = [vocab.numericalize(s) for s in val_tokens]\n",
        "  test_texts_numz = [vocab.numericalize(s) for s in test_tokens]\n",
        "\n",
        "  print(val_texts_numz[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[38, 9, 2439, 2262, 89, 10, 61, 212, 25, 193, 17, 317, 26, 93, 14, 32, 11, 1167, 2154, 237, 1113, 10, 0, 3097, 24, 114, 855, 3776], [9, 3545, 32, 27, 11, 1498, 263, 26, 10, 2496, 21, 12, 11, 57, 1320, 411, 9873, 13, 10, 84, 27, 38, 41, 712], [14, 28, 57, 16, 11, 0, 62, 3545, 69, 32, 57, 2708, 12, 66], [14, 0, 49, 958, 22, 24, 1078, 384, 13, 10, 1951, 14, 50, 75, 9, 142, 48, 17, 384, 15, 846, 20], [9, 271, 32, 124, 12, 243, 13, 878, 13, 10, 60, 58, 2783, 3268, 9, 3776, 109, 9, 4439, 1763, 1009, 148, 570, 37, 2050, 14, 12, 44], [385, 73, 25, 2249, 64, 459, 122, 79, 10, 25, 27, 1476, 21, 13, 25, 27, 25, 9, 122, 2617, 38, 85, 79], [14, 15, 124, 12, 705, 9, 122, 439, 82, 8514, 9, 112, 410, 23, 9, 195, 13, 6480, 9, 534, 16, 787, 2271, 18, 2901, 126], [9, 135, 205, 313, 34, 124, 12, 1116, 18, 73, 26, 10, 84, 1408, 16, 21, 4393, 13, 4008, 3178, 48, 10, 227, 14, 20, 24, 259, 37, 537], [93, 16, 21, 135, 205, 14, 392, 68, 11, 339, 12, 287, 9, 3776, 109, 9, 415, 13, 10, 75, 27, 998, 14, 543, 10, 96, 23, 11, 733], [6337, 21, 14, 28, 1317, 10, 96, 74, 7656, 20, 57, 6760, 1191], [10, 60, 68, 39, 470, 22, 11, 221, 62, 275, 733, 13, 32, 11, 178, 0, 440, 10, 1675, 9, 1294, 32, 5209, 43, 9, 72, 245, 16, 9, 733], [20, 1145, 10, 55, 9, 257, 1219, 1121, 16, 44, 13, 80, 29, 341, 55, 10, 25, 11, 0, 0, 43, 24, 415], [10, 91, 118, 17, 230]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eof16bhAci-C",
        "outputId": "a7fd245d-83c4-4331-88f5-5800099f42c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Validate tokenisation and vocab building works\n",
        "if split_sentences:\n",
        "  print(val_sents_tokens[0])\n",
        "  print([vocab.itos[i] for i in val_texts_numz[0][0]])\n",
        "else:\n",
        "  print(val_tokens[0])\n",
        "  print([vocab.itos[i] for i in val_texts_numz[0]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['at', 'the', 'suggested', 'retail', 'price', 'i', 'would', 'never', 'have', 'made', 'this', 'purchase', 'but', 'because', 'it', 'was', 'a', 'reasonable', 'gold', 'box', 'offer', 'i', 'leapt', 'toward', 'my', 'first', 'bluetooth', 'earpiece']\n",
            "['at', 'the', 'suggested', 'retail', 'price', 'i', 'would', 'never', 'have', 'made', 'this', 'purchase', 'but', 'because', 'it', 'was', 'a', 'reasonable', 'gold', 'box', 'offer', 'i', 'xxunk', 'toward', 'my', 'first', 'bluetooth', 'earpiece']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A3WYVuK-vgr",
        "colab_type": "text"
      },
      "source": [
        "We restrict the max sentence length and document length using 80th quantile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jcGPGZUudnLE",
        "outputId": "f9caa534-cbec-4e5a-a44f-23de1aa8de10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "q=0.8\n",
        "\n",
        "if split_sentences:\n",
        "  train_sents_length = [len(s) for s in train_sents_tokens]\n",
        "  maxlen_sent = int(np.quantile(train_sents_length, q=q))\n",
        "  maxlen_doc  = int(np.quantile(train_texts_length, q=q))\n",
        "\n",
        "  print(\"Max Sentence Length: {}\".format(maxlen_sent))\n",
        "  print(\"Max Document Length: {}\".format(maxlen_doc))\n",
        "else:\n",
        "  train_length = [len(s) for s in train_tokens]\n",
        "  maxlen_sent  = int(np.quantile(train_length, q=q))\n",
        "  maxlen_doc = 1\n",
        "  print(\"Max Review Length: {}\".format(maxlen_sent))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Sentence Length: 24\n",
            "Max Document Length: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f7DZtoSmH_Or",
        "outputId": "f04b21a6-a77e-4a1a-dc2f-74bbd04bd779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "# Distribution of sentence and document length\n",
        "\n",
        "if split_sentences:\n",
        "  f, (ax1, ax2) = plt.subplots(1,2, figsize=(12,3))\n",
        "  ax1.hist(train_sents_length, range=(0, 100), bins=100);\n",
        "  ax2.hist(train_texts_length, range=(0, 100), bins=100);\n",
        "  ax1.title.set_text('Sentence Length');\n",
        "  ax2.title.set_text('Document Length');\n",
        "else:\n",
        "  f, (ax1) = plt.subplots(1,1, figsize=(12,3))\n",
        "  ax1.hist(train_length, range=(0, 1000), bins=100);\n",
        "  ax1.title.set_text('Review Length');"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtIAAADSCAYAAABn9SeDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3df7RdZX3n8fdHfom/+KEpjQkYqowWXSNqCjg6M1YUAtVCZ1UL7ZTUYZp2Cq1tbTW0nUGtdLCrlapjaVFSwFqRopYUUZoiro7TEglK+aklBZVkBYj81GpV6Hf+2M+FYzw39+bcc3PPPef9Wmuvu/ezn73Ps+8m3/vlOc9+dqoKSZIkSbvmCQvdAEmSJGkxMpGWJEmSBmAiLUmSJA3ARFqSJEkagIm0JEmSNAATaUmSJGkAJtLSIpdkRZJKsudCt0WSNHvG78XPRFpDleTlSf4+yUNJ7k/y/5L8yBDO+3NJPjuMNg5Tki8nedW4f6ak8dFiyLeSfD3Jgy1m/2KSsckJkrw1yZ/PUMf4rTkbm380WnhJngZcAbwXOBBYBrwN+PZCtkuS9H1eW1VPBZ4FnAO8BbhgYZskLT4m0hqmfwdQVR+uqker6ltV9TdVdeNUhST/LcltSR5IclWSZ/Xsq9YrcnvrJXlfOj8M/Anw0iTfSPJgq79Pkj9I8tUk9yT5kyT7tn2vSLIlyZuS3JtkW5I39HzWvkn+MMlXWu/5Z3uOPbr10DyY5B+TvGJXfxFJnpBkbZJ/TnJfkkuTHNj2TX2Vt7q1/WtJfnuHtl3Ufke3JXlzki1t3weBQ4C/br+LN/d87M/0O58kTaeqHqqq9cBPAauTvAAgyX5JLk6yvcXJ3+ntsU7y8y0+fT3JrUle3MoryXN66l2Y5B1tfSouv7knLp+U5IQk/9S+xfytnmMHiqNJVgG/BfxUi5P/uCu/E+O3doWJtIbpn4BHWxA5PskBvTuTnEgX3P4LsAT4v8CHdzjHa4AfAf498HrguKq6DfhF4B+q6ilVtX+rew5d8n4E8By6HvD/1XOuHwT2a+WnAe/radMfAC8B/gNd7/mbgX9Lsgz4BPCOVv4bwEeTLNnF38UvAycB/xl4JvAA8L4d6rwceC5wDPC/2v8wAJwFrAB+CHg18F+nDqiqnwW+Steb9JSq+v1ZnE+SdqqqPgdsAf5jK3ovXfz8Ibo4dirwBoAkrwPe2sqeBvw4cN8sP+oHgSfyeLx+P12Me0n77P+Z5NBWd6A4WlWfAn4P+EiLky+cZdumGL81e1Xl4jK0Bfhh4EK6gPwIsB44qO37JHBaT90nAN8EntW2C3h5z/5LgbVt/eeAz/bsC/AvwLN7yl4K3NnWXwF8C9izZ/+9wNHtc78FvLBP+98CfHCHsquA1dNc75eBV/Upvw04pmd7KfBdYE+6IFvA8p79nwNObut30P0PxNS+/w5sme4zZzqfi4uLS++yk7h1LfDbwB7Ad4DDe/b9AvCZtn4V8MZpzl3Ac3q2LwTe0dan4vIebfuprf5RPfWvB05q63OJo28F/nzA34Px22XWi0+Jaqiq6z3+OYAkzwP+HPgj4BS6sXjvTvKHPYeErmfiK2377p593wSeMs1HLQGeBFyfpPdce/TUua+qHulzvmfQ9Yj8c5/zPgt4XZLX9pTtBVwzTTum8yzg40n+rafsUeCgnu3prvWZwF09+3rXd2a2vztJ6mcZcD9djNyLx+MybX1ZWz+Y/vFzNu6rqkfb+rfaz3t69n+Lx2PXXOLoXBi/NWsO7dC8qaov0vVGvKAV3QX8QlXt37PsW1V/P5vT7bD9NbqA+/yec+1XVbMJPl8D/hV4dp99d9H1SPe28clVdc4szrvjeY7f4TxPrKqtszh2G7C8Z/vgHfbv+LuQpDlJN7vSMuCzdDHyu3QJ5ZRDgKn4dRf94yd0SeCTerZ/cA7NmkscnUucNH5r1kykNTRJnpfu4b7lbftgup7oa1uVPwHOTPL8tn+/NtZuNu4BlifZG6Cq/o1ubN25SX6gnW9ZkuNmOlE7dh3wriTPTLJHkpcm2YeuB/21SY5r5U9sD8gs38kp92r1ppY927WenfYwZZIlbYz4bFxK93s6oI3ZPqPP7+KHZnkuSZpWkqcleQ1wCd1QiJtaj/GldDHsqS2O/TpdfAT4APAbSV6SznPy+IPjNwA/3eLnKrpxxoOaSxy9B1iRmaf0M35rTkykNUxfB44CNib5F7oE+mbgTQBV9XHgncAlSR5u+46f5bk/DdwC3J3ka63sLcBm4Np2vr+le1hjNn4DuAm4ju6rzHcCT6iqu4CphyK30/VM/CY7/7dyJV3v+NTyVuDddOPD/ybJ1+l+F0fNsm1vpxtjfme7psv43ikE/zfwO+lmFfmNWZ5Tknr9dYtNd9GNi34X7WHC5pfpnkO5g66X+i/oOiCoqr8Ezm5lXwf+iu7hbIA3Aq8FHgR+pu0b1Fzi6F+2n/cl+fxO6hm/NSep8lsGaZQl+R90D57MpWdHkrSbGb/Hnz3S0ohJsjTJy9pcps+l69H/+EK3S5K0c8bvyeOsHdLo2Rv4U+BQuq9HLwH+eEFbJEmaDeP3hHFohyRJkjQAh3ZIkiRJAzCRliRJkgawaMdIP+MZz6gVK1YsdDMkaZddf/31X6uqJQvdjt3JmC1psdpZzF60ifSKFSvYtGnTQjdDknZZkq/MXGu8GLMlLVY7i9kO7ZAkSZIGYCItSZIkDcBEWpIkSRqAibQkSZI0ABNpSZIkaQAzJtJJnpjkc0n+McktSd7Wyi9McmeSG9pyRCtPkvck2ZzkxiQv7jnX6iS3t2V1T/lLktzUjnlPkszHxY6KFWs/8dgiSZod46akUTOb6e++Dbyyqr6RZC/gs0k+2fb9ZlVdtkP944HD2nIUcB5wVJIDgbOAlUAB1ydZX1UPtDo/D2wErgRWAZ9EkiRJGlEzJtJVVcA32uZebamdHHIicHE77tok+ydZCrwC2FBV9wMk2QCsSvIZ4GlVdW0rvxg4iTFLpKfrRZkq//I5P7Y7myNJkqQ5mtUY6SR7JLkBuJcuGd7Ydp3dhm+cm2SfVrYMuKvn8C2tbGflW/qU92vHmiSbkmzavn37bJq+aDjcQ5IkaXGZVSJdVY9W1RHAcuDIJC8AzgSeB/wIcCDwlnlr5ePtOL+qVlbVyiVLJurtupI0a0nWJbk3yc09ZQcm2dCeUdmQ5IBW7nMtkjSgXZq1o6oeBK4BVlXVtup8G/gz4MhWbStwcM9hy1vZzsqX9ymXJA3mQrpnTXqtBa6uqsOAq9s2fO9zLWvonlmh57mWo+ji+1lTyTePP9cyddyOnyVJE2E2s3YsSbJ/W98XeDXwxTbumdYTcRIw1fOxHji19XIcDTxUVduAq4BjkxzQgvGxwFVt38NJjm7nOhW4fLiXuTAcriFpIVTV3wH371B8InBRW7+ILm5PlV/cOkauBaaeazmO9lxLeyh86rmWpbTnWtqzMBf3nEuSJspsZu1YClyUZA+6xPvSqroiyaeTLAEC3AD8Yqt/JXACsBn4JvAGgKq6P8nvAte1em+fevAQ+CW6HpR96R4yHKsHDSVpBBzUOi4A7gYOauvz+lwLXS83hxxyyBybL0mjZzazdtwIvKhP+SunqV/A6dPsWwes61O+CXjBTG2ZFL092M7mIWnYqqqS7Gz2pWF9zvnA+QArV66c98+TpN3NNxtK0mS4p2dI3lK6WZjA51okaWCzGdqhXeB4aEkjaj2wGjin/by8p/yMJJfQPVj4UFVtS3IV8Hs9DxgeC5zZhuk93J6B2Uj3XMt7d+eFSNKoMJGWpDGT5MN0L8F6RpItdLNvnANcmuQ04CvA61t1n2uRpAGZSEvSmKmqU6bZdUyfuj7XIkkDcoy0JEmSNAATaUmSJGkADu0YcU6FJ0mSNJrskZYkSZIGYCItSZIkDcChHUPi/NGSJEmTxR5pSZIkaQAm0pIkSdIAZkykkzwxyeeS/GOSW5K8rZUfmmRjks1JPpJk71a+T9ve3Pav6DnXma38S0mO6ylf1co2J1k7/MscDyvWfsIhJJIkSSNiNj3S3wZeWVUvBI4AViU5GngncG5VPQd4ADit1T8NeKCVn9vqkeRw4GTg+cAq4I+T7JFkD+B9wPHA4cApra4kSZI0smZMpKvzjba5V1sKeCVwWSu/CDiprZ/Ytmn7j0mSVn5JVX27qu4ENgNHtmVzVd1RVd8BLml1JUmSpJE1qzHSref4BuBeYAPwz8CDVfVIq7IFWNbWlwF3AbT9DwFP7y3f4ZjpyiVJkqSRNatEuqoeraojgOV0PcjPm9dWTSPJmiSbkmzavn37QjRBkiRJAnZx1o6qehC4BngpsH+SqXmolwNb2/pW4GCAtn8/4L7e8h2Oma683+efX1Urq2rlkiVLdqXpkiRJ0lDN+EKWJEuA71bVg0n2BV5N9wDhNcBP0o1pXg1c3g5Z37b/oe3/dFVVkvXAXyR5F/BM4DDgc0CAw5IcSpdAnwz89PAucf44g4YkSdLkms2bDZcCF7XZNZ4AXFpVVyS5FbgkyTuALwAXtPoXAB9Mshm4ny4xpqpuSXIpcCvwCHB6VT0KkOQM4CpgD2BdVd0ytCuUJEmS5sGMiXRV3Qi8qE/5HXTjpXcs/1fgddOc62zg7D7lVwJXzqK9kiRJ0kjwzYaSJEnSAGYztEMjpnds9pfP+bEFbIkkSdLkskdakiRJGoCJtCRNkCS/luSWJDcn+XCSJyY5NMnGJJuTfCTJ3q3uPm17c9u/ouc8Z7byLyU5bqGuR5IWkom0JE2IJMuAXwFWVtUL6GZKOpluStNzq+o5wAPAae2Q04AHWvm5rR5JDm/HPR9YBfxxm9lJkiaKibQkTZY9gX3bC7OeBGwDXglc1vZfBJzU1k9s27T9xyRJK7+kqr5dVXcCm+kzi5MkjTsTaUmaEFW1FfgD4Kt0CfRDwPXAg1X1SKu2BVjW1pcBd7VjH2n1n95b3ucYSZoYztqxyDmDh6TZSnIAXW/yocCDwF/SDc2Yr89bA6wBOOSQQ4Z2XuOepFFhj7QkTY5XAXdW1faq+i7wMeBlwP5tqAfAcmBrW98KHAzQ9u8H3Ndb3ueYx1TV+VW1sqpWLlmyZD6uR5IWlD3Su6i3J0SSFpmvAkcneRLwLeAYYBNwDfCTwCXAauDyVn992/6Htv/TVVVJ1gN/keRdwDOBw4DP7c4LkaRRYCItSROiqjYmuQz4PPAI8AXgfOATwCVJ3tHKLmiHXAB8MMlm4H66mTqoqluSXArc2s5zelU9ulsvRpJGgIm0JE2QqjoLOGuH4jvoM+tGVf0r8LppznM2cPbQGyhJi4iJtCRp0fLBQ0kLacaHDZMcnOSaJLe2t2G9sZW/NcnWJDe05YSeY/q+8SrJqla2OcnanvK+b9WSJEmSRtVsZu14BHhTVR0OHA2c3t5qBd2bsI5oy5Uw/Ruv2luv3gccDxwOnNJznuneqiVJkiSNpBkT6araVlWfb+tfB25j5xPvT/fGqyOBzVV1R1V9h+7p8BPbW7Kme6uWJEmSNJJ2aR7pJCuAFwEbW9EZSW5Msq5N9A/Tv/FquvKnM/1btSRJkqSRNOtEOslTgI8Cv1pVDwPnAc8GjqB71ewfzksLv7cNa5JsSrJp+/bt8/1xi86KtZ94bJEkSdL8mtWsHUn2okuiP1RVHwOoqnt69r8fuKJt7uyNV/3K76O9Vav1Svd9Q1b7zPPp5jxl5cqVNZu2D4OJqSRJknY0m1k7Qjcp/21V9a6e8qU91X4CuLmtrwdOTrJPkkN5/I1X1wGHtRk69qZ7IHF9VRWPv1ULvvetWpIkSdJImk2P9MuAnwVuSnJDK/stulk3jgAK+DLwC7DzN14lOQO4CtgDWFdVt7TzvYX+b9WSJEmSRtKMiXRVfRZIn11X7uSYvm+8alPkfd9xVdX3rVqSJEnSqNqlWTskSZIkdUykx5Szd0iSJM0vE2lJkiRpACbSkiRJ0gBMpCVJkqQBmEhLkiRJAzCRliRJkgZgIi1JkiQNYDZvNpQkaeT1Tvn55XN+bAFbImlSmEiPOf+wSJIkzQ+HdkiSJEkDMJGWpAmSZP8klyX5YpLbkrw0yYFJNiS5vf08oNVNkvck2ZzkxiQv7jnP6lb/9iSrF+6KJGnhmEhL0mR5N/Cpqnoe8ELgNmAtcHVVHQZc3bYBjgcOa8sa4DyAJAcCZwFHAUcCZ00l35I0SWZMpJMcnOSaJLcmuSXJG1v50HowkrwkyU3tmPckyXxcrCRNsiT7Af8JuACgqr5TVQ8CJwIXtWoXASe19ROBi6tzLbB/kqXAccCGqrq/qh4ANgCrduOlSNJImE2P9CPAm6rqcOBo4PQkhzPcHozzgJ/vOc6ALEnDdyiwHfizJF9I8oEkTwYOqqptrc7dwEFtfRlwV8/xW1rZdOWSNFFmTKSraltVfb6tf53ua8BlDKkHo+17WlVdW1UFXNxzrgW1Yu0nvmfWC0la5PYEXgycV1UvAv6FxztBAGhxuIbxYUnWJNmUZNP27duHcUpJGim7NEY6yQrgRcBGhteDsayt71guSRquLcCWqtrYti+jS6zvaZ0atJ/3tv1bgYN7jl/eyqYr/x5VdX5VrayqlUuWLBnqhUjSKJh1Ip3kKcBHgV+tqod79w2zB2OGNti7IUkDqqq7gbuSPLcVHQPcCqwHpp5bWQ1c3tbXA6e2Z1+OBh5qHShXAccmOaAN0Tu2lUnSRJnVC1mS7EWXRH+oqj7Wiu9JsrSqtu1CD8Yrdij/TCtf3qf+96mq84HzAVauXDnvibskjaFfBj6UZG/gDuANdJ0qlyY5DfgK8PpW90rgBGAz8M1Wl6q6P8nvAte1em+vqvt33yVI0miYMZFuM2hcANxWVe/q2TXVg3EO39+DcUaSS+geLHyoJdtXAb/X84DhscCZLSA/3Ho7NgKnAu8dwrVpB77lUFJV3QCs7LPrmD51Czh9mvOsA9YNt3WStLjMpkf6ZcDPAjcluaGV/RZdAj2sHoxfAi4E9gU+2RZJkiRpZM2YSFfVZ4Hp5nUeSg9GVW0CXjBTWyRJkqRR4ZsNJUmSpAGYSEuSJEkDMJGWJI0dX6glaXcwkZYkSZIGYCItSZIkDWBWL2TR+HFOaUmSpLmxR1qSJEkagIm0JEmSNAATaUmSJGkAJtKSJEnSAEykJUmSpAGYSEuSJEkDcPo7PTYVntPgSRo3TvUpaT7N2COdZF2Se5Pc3FP21iRbk9zQlhN69p2ZZHOSLyU5rqd8VSvbnGRtT/mhSTa28o8k2XuYFyhJkiTNh9kM7bgQWNWn/NyqOqItVwIkORw4GXh+O+aPk+yRZA/gfcDxwOHAKa0uwDvbuZ4DPACcNpcLkiRJknaHGRPpqvo74P5Znu9E4JKq+nZV3QlsBo5sy+aquqOqvgNcApyYJMArgcva8RcBJ+3iNUiSJEm73VzGSJ+R5FRgE/CmqnoAWAZc21NnSysDuGuH8qOApwMPVtUjfep/nyRrgDUAhxxyyByaPr3e8XSSJEnSdAadteM84NnAEcA24A+H1qKdqKrzq2plVa1csmTJ7vhISZIkqa+BeqSr6p6p9STvB65om1uBg3uqLm9lTFN+H7B/kj1br3RvfUmSJGlkDdQjnWRpz+ZPAFMzeqwHTk6yT5JDgcOAzwHXAYe1GTr2pnsgcX1VFXAN8JPt+NXA5YO0SXO3Yu0nHlskjaf2APgXklzRtvvOnNTi+Eda+cYkK3rO0Xd2JkmaNLOZ/u7DwD8Az02yJclpwO8nuSnJjcCPAr8GUFW3AJcCtwKfAk6vqkdbb/MZwFXAbcClrS7AW4BfT7KZbsz0BUO9QklSrzfSxeEp082cdBrwQCs/t9Wbdnam3dR2SRopMw7tqKpT+hRPm+xW1dnA2X3KrwSu7FN+B92sHgvGHlhJkyDJcuDH6GL0r/fMnPTTrcpFwFvpnoM5sa1DN7PS/2n1H5udCbizdYIcSdfhIkkTxTcbStLk+CPgzcBT2/bOZk5aRpttqaoeSfJQq7+z2Zm+x+6YaWlX+JZDScM26KwdkqRFJMlrgHur6vrd9ZnOtCRp3NkjLUmT4WXAjyc5AXgi8DTg3Uw/c9LULExbkuwJ7Ec309LOZmeSpIlij7T6cgYPabxU1ZlVtbyqVtA9LPjpqvoZpp85aX3bpu3/dJtpabrZmSRp4tgjLUmT7S3AJUneAXyBxx8mvwD4YHuY8H665JuquiXJ1OxMj9BmZ9r9zZakhWciLUkTpqo+A3ymrfedOamq/hV43TTH952dSZImjUM7JEmSpAGYSEuSJEkDMJGWJEmSBuAYaUnSxPHlLJKGwURaM/IPjiRJ0vdzaIckSZI0ABNpSZIkaQAzJtJJ1iW5N8nNPWUHJtmQ5Pb284BWniTvSbI5yY1JXtxzzOpW//Ykq3vKX5LkpnbMe5Jk2BcpSZIkDdtseqQvBFbtULYWuLqqDgOubtsAx9O9LvYwYA1wHnSJN3AWcBTdxP9nTSXfrc7P9xy342dphPjqcEmSpM6MiXRV/R3d62F7nQhc1NYvAk7qKb+4OtcC+ydZChwHbKiq+6vqAWADsKrte1pVXVtVBVzccy5JkuadnQOSBjXoGOmDqmpbW78bOKitLwPu6qm3pZXtrHxLn/K+kqxJsinJpu3btw/YdEmSJGnu5vywYetJriG0ZTafdX5VrayqlUuWLNkdHylJkiT1NWgifU8blkH7eW8r3woc3FNveSvbWfnyPuWSJEnSSBs0kV4PTM28sRq4vKf81DZ7x9HAQ20IyFXAsUkOaA8ZHgtc1fY9nOToNlvHqT3n0ohzXKEkSZpkM77ZMMmHgVcAz0iyhW72jXOAS5OcBnwFeH2rfiVwArAZ+CbwBoCquj/J7wLXtXpvr6qpBxh/iW5mkH2BT7Zl3kwlfr6hT5LUy7e4StpVMybSVXXKNLuO6VO3gNOnOc86YF2f8k3AC2ZqhyRJkjRKZkykx5VDEiRJkjQXE5tIa3j8OlSSJE2iOU9/J0mSJE0iE2lJkiRpAA7tkCRpBw5ZkzQb9khL0oRIcnCSa5LcmuSWJG9s5Qcm2ZDk9vbzgFaeJO9JsjnJjUle3HOu1a3+7UlWT/eZkjTO7JHWUNmLI420R4A3VdXnkzwVuD7JBuDngKur6pwka4G1wFuA44HD2nIUcB5wVJID6d4psBKodp71VfXAbr8iSVpA9khL0oSoqm1V9fm2/nXgNmAZcCJwUat2EXBSWz8RuLg61wL7J1kKHAdsqKr7W/K8AVi1Gy9FkkaCPdKSNIGSrABeBGwEDqqqbW3X3cBBbX0ZcFfPYVta2XTlO37GGmANwCGHHDK8xu9mftMmaTom0po3/vGRRlOSpwAfBX61qh5O8ti+qqokNYzPqarzgfMBVq5cOZRzStIocWiHJE2QJHvRJdEfqqqPteJ72pAN2s97W/lW4OCew5e3sunKJWmimEhL0oRI1/V8AXBbVb2rZ9d6YGrmjdXA5T3lp7bZO44GHmpDQK4Cjk1yQJvh49hWNvZWrP3EY4skzSmRTvLlJDcluSHJplbmNEr6Pv7hkUbCy4CfBV7Z4vYNSU4AzgFeneR24FVtG+BK4A5gM/B+4JcAqup+4HeB69ry9lYmSRNlGGOkf7SqvtazvRanUZKkkVNVnwUyze5j+tQv4PRpzrUOWDe81knS4jMfQzucRkmSJEljb6490gX8TXvC+0/bE9rzMo2SxoMzeUgaF1PxzFgmTa65JtIvr6qtSX4A2JDki707hzmNEozPnKSSJEla/OaUSFfV1vbz3iQfB46kTaNUVdt2YRqlV+xQ/plpPs85SceIvdOSxoGxTJpcA4+RTvLkJE+dWqeb/uhmnEZJkiRJE2AuPdIHAR9vb8TaE/iLqvpUkuuAS5OcBnwFeH2rfyVwAt00St8E3gDdNEpJpqZRAqdRkiQtUvZOS5Nl4ES6qu4AXtin/D6cRkm7yD8+kiRpsfHNhho5vrxFkiQtBsN4IYskSdqB37RJ489EWiPLP0KSxoVzTkvjyURakqTdxA4CabyYSGtR8I+PJEkaNSbSWnRMqiWNg+keqjauSYuHs3ZIkiRJA7BHWouavdOSxo0PJkqLh4m0xoZ/fCSNE4d+SKPPRFpjx15qSePMGCeNDhNpjTX/4EgaZzO9Bda4J80vE2lNjH5/cPwjI2mc2ZkgzS8TaU00xyBKmhT2XkvDNzKJdJJVwLuBPYAPVNU5C9wkTbCZ/uD08o+PJpExe/yYaEu7biQS6SR7AO8DXg1sAa5Lsr6qbl3YlkkzM+nWpDFmT6bZfIPXb/Ykh5donI1EIg0cCWyuqjsAklwCnAgYlDVWdiXp7uUfH40YY7Ye0y+uTRfrBo2BvabioQm6RsGoJNLLgLt6trcARy1QW6SRM4w/PovNqP6xHLX2LBBjthbMriTui1m/GNhv/4515it2+q6G/kYlkZ6VJGuANW3zG0m+NMBpngF8bXitGile2+I1ztc30LXlnbMrW0h558D37VnDbssoMmbPyGtbvOb9+maKd9PtH0Ls3Om1jVoc3kVDj9mjkkhvBQ7u2V7eyr5HVZ0PnD+XD0qyqapWzuUco8prW7zG+fq8trFkzB4Cr23xGufr89p2zROGebI5uA44LMmhSfYGTgbWL3CbJEn9GbMliRHpka6qR5KcAVxFN5XSuqq6ZYGbJUnqw5gtSZ2RSKQBqupK4Mrd8FFz+ppxxHlti9c4X5/XNoaM2UPhtS1e43x9XtsuSFUN+5ySJEnS2BuVMdKSJEnSojIxiXSSVUm+lGRzkrUL3Z65SHJwkmuS3JrkliRvbOUHJtmQ5Pb284CFbuugkuyR5AtJrmjbhybZ2O7fR9oDTotSkv2TXJbki0luS/LScbl3SX6t/Td5c5IPJ3niYr53SdYluTfJzT1lfe9VOu9p13ljkhcvXMsXv3GK2WDcXmz/9nuNc8yG8YrbCxGzJyKRzuOvsz0eOBw4JcnhC9uqOXkEeFNVHQ4cDZzermctcHVVHQZc3bYXqzcCt/VsvxM4t6qeAzwAnLYgrRqOdwOfqqrnAS+ku85Ff++SLAN+BVhZVS+gewjtZBb3vbsQWLVD2XT36kQiCC0AAAMnSURBVHjgsLasAc7bTW0cO2MYs8G4vdj+7fcay5gNYxm3L2R3x+yqGvsFeClwVc/2mcCZC92uIV7f5cCrgS8BS1vZUuBLC922Aa9nefuP/ZXAFUDoJlDfs9/9XEwLsB9wJ+35hJ7yRX/vePxtdwfSPch8BXDcYr93wArg5pnuFfCnwCn96rns8u98rGN2uybj9iJYxjlmt7aPXdze3TF7Inqk6f8622UL1JahSrICeBGwETioqra1XXcDBy1Qs+bqj4A3A//Wtp8OPFhVj7TtxXz/DgW2A3/WvgL9QJInMwb3rqq2An8AfBXYBjwEXM/43Lsp092rsY0zC2Csf5fG7UVlbGM2TEzcnteYPSmJ9FhK8hTgo8CvVtXDvfuq+9+rRTclS5LXAPdW1fUL3ZZ5sifwYuC8qnoR8C/s8JXgIr53BwAn0v3heSbwZL7/K7axsljvlRaOcXvRGduYDZMXt+fjXk1KIj2r19kuJkn2ogvGH6qqj7Xie5IsbfuXAvcuVPvm4GXAjyf5MnAJ3deE7wb2TzI17/livn9bgC1VtbFtX0YXpMfh3r0KuLOqtlfVd4GP0d3Pcbl3U6a7V2MXZxbQWP4ujduL8h6Oc8yGyYjb8xqzJyWRHqvX2SYJcAFwW1W9q2fXemB1W19NNwZvUamqM6tqeVWtoLtPn66qnwGuAX6yVVuU1wZQVXcDdyV5bis6BriVMbh3dF8NHp3kSe2/0alrG4t712O6e7UeOLU9CX408FDP14naNWMVs8G4zeK9tnGO2TAZcXt+Y/ZCDwrfXQtwAvBPwD8Dv73Q7Znjtbyc7quJG4Eb2nIC3Zi0q4Hbgb8FDlzots7xOl8BXNHWfwj4HLAZ+Etgn4Vu3xyu6whgU7t/fwUcMC73Dngb8EXgZuCDwD6L+d4BH6YbN/hdup6p06a7V3QPV72vxZib6J6CX/BrWKzLOMXsdj3G7UX0b3+HaxrbmN2ub2zi9kLEbN9sKEmSJA1gUoZ2SJIkSUNlIi1JkiQNwERakiRJGoCJtCRJkjQAE2lJkiRpACbSkiRJ0gBMpCVJkqQBmEhLkiRJA/j/x9OBM4g89pEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anQaBhOLdxrk",
        "colab": {}
      },
      "source": [
        "def pad_sequences(seq, maxlen, pad_first=False, pad_idx=1):\n",
        "    if len(seq) >= maxlen:\n",
        "        res = np.array(seq[-maxlen:]).astype(\"int32\")\n",
        "        return res\n",
        "    else:\n",
        "        res = np.zeros(maxlen, dtype=\"int32\") + pad_idx\n",
        "        if pad_first:\n",
        "            res[-len(seq) :] = seq\n",
        "        else:\n",
        "            res[: len(seq) :] = seq\n",
        "        return res\n",
        "\n",
        "\n",
        "def pad_nested_sequences(\n",
        "    seq, maxlen_sent, maxlen_doc, pad_sent_first=False, pad_doc_first=False, pad_idx=1\n",
        "):\n",
        "    seq = [s for s in seq if len(s) >= 1]\n",
        "    if len(seq) == 0:\n",
        "        return np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "    seq = [pad_sequences(s, maxlen_sent, pad_sent_first, pad_idx) for s in seq]\n",
        "    if len(seq) >= maxlen_doc:\n",
        "        return np.array(seq[:maxlen_doc])\n",
        "    else:\n",
        "        res = np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "        if pad_doc_first:\n",
        "            res[-len(seq) :] = seq\n",
        "        else:\n",
        "            res[: len(seq) :] = seq\n",
        "        return res\n",
        "\n",
        "def pad_single_sequence(\n",
        "    seq, maxlen, pad_sent_first=False, pad_doc_first=False, pad_idx=1\n",
        "):\n",
        "    # seq = [s for s in seq if len(s) >= 1]\n",
        "    if len(seq) == 0:\n",
        "        return np.array([pad_idx] * maxlen).astype(\"int32\")\n",
        "    seq = pad_sequences(seq, maxlen, pad_sent_first, pad_idx)\n",
        "\n",
        "    if len(seq) >= maxlen:\n",
        "        return np.array(seq[:maxlen])\n",
        "    else:\n",
        "        res = np.array([pad_idx] * maxlen).astype(\"int32\")\n",
        "        if pad_doc_first:\n",
        "            res[-len(seq) :] = seq\n",
        "        else:\n",
        "            res[: len(seq) :] = seq\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_EydKhySdyMi",
        "outputId": "09c1611a-ec8f-49f5-abf5-a0caf758e3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "if split_sentences:\n",
        "  train_padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in train_texts_numz], axis=0)\n",
        "  val_padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in val_texts_numz], axis=0)\n",
        "  test_padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in test_texts_numz], axis=0)\n",
        "\n",
        "else:\n",
        "  train_padded_texts = np.stack([pad_single_sequence(r, maxlen_sent) for r in train_texts_numz], axis=0)\n",
        "  val_padded_texts = np.stack([pad_single_sequence(r, maxlen_sent) for r in val_texts_numz], axis=0)\n",
        "  test_padded_texts = np.stack([pad_single_sequence(r, maxlen_sent) for r in test_texts_numz], axis=0)\n",
        "\n",
        "print(val_padded_texts.shape)\n",
        "print(val_padded_texts[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 15, 24)\n",
            "[[  89   10   61  212 ...   24  114  855 3776]\n",
            " [   9 3545   32   27 ...   27   38   41  712]\n",
            " [  14   28   57   16 ...    1    1    1    1]\n",
            " [  14    0   49  958 ...  846   20    1    1]\n",
            " ...\n",
            " [  20 1145   10   55 ...   43   24  415    1]\n",
            " [  10   91  118   17 ...    1    1    1    1]\n",
            " [   1    1    1    1 ...    1    1    1    1]\n",
            " [   1    1    1    1 ...    1    1    1    1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UnrQpAuJ9kEH"
      },
      "source": [
        "## Load into PyTorch DataLoader and Create Embeddings Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WqvNRKl_j6iR",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QDytIlAFfcth",
        "outputId": "169a7c03-76f0-4f50-ddc5-da2bbd99528a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_set = TensorDataset(torch.from_numpy(train_padded_texts), torch.from_numpy(train[target].to_numpy()))\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
        "val_set = TensorDataset(torch.from_numpy(val_padded_texts), torch.from_numpy(val[target].to_numpy()))\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
        "test_set = TensorDataset(torch.from_numpy(test_padded_texts), torch.from_numpy(test[target].to_numpy()))\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n",
        "\n",
        "next(iter(train_loader))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[ 424,   66,  531,  ...,   27, 1435,   19],\n",
              "          [  24, 1001,   64,  ..., 1147,  177,  316],\n",
              "          [  98,   21,   21,  ...,    1,    1,    1],\n",
              "          ...,\n",
              "          [  48,   10,  940,  ...,    9, 4246,    1],\n",
              "          [ 184,   10,  203,  ...,    1,    1,    1],\n",
              "          [ 825, 6018,   15,  ...,   20,    9,  299]],\n",
              " \n",
              "         [[  51,  314,   11,  ...,    1,    1,    1],\n",
              "          [ 191,   29,  819,  ...,    1,    1,    1],\n",
              "          [  19,  564,   18,  ...,    1,    1,    1],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         [[  10,  240,   17,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[  17,   15,   11,  ...,    1,    1,    1],\n",
              "          [ 762,   46,  402,  ...,    1,    1,    1],\n",
              "          [   9, 3390,  257,  ...,    1,    1,    1],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         [[  10,   88,  632,  ...,    1,    1,    1],\n",
              "          [  10,   88,  102,  ...,    1,    1,    1],\n",
              "          [ 149,   23,    9,  ...,   12, 1629,   64],\n",
              "          ...,\n",
              "          [ 113,  280,   28,  ...,  193,   20, 2230],\n",
              "          [  54,  117,  164,  ...,  225,   18,  158],\n",
              "          [ 857,   26,   24,  ...,  110,  251,  431]],\n",
              " \n",
              "         [[  32,  216,   18,  ...,  590,  933,  682],\n",
              "          [ 163,   10,   25,  ...,    1,    1,    1],\n",
              "          [  10,   84,   27,  ...,    1,    1,    1],\n",
              "          ...,\n",
              "          [  30,   22,  116,  ...,    1,    1,    1],\n",
              "          [  16,   96,  111,  ...,   22,    0, 1766],\n",
              "          [  73, 1766,   48,  ...,   82,  202, 1497]]], dtype=torch.int32),\n",
              " tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "         1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yH5FYPLRy8RH",
        "colab": {}
      },
      "source": [
        "def build_embeddings_matrix(vocab, word_vectors_path, verbose=1):\n",
        "    r\"\"\"\n",
        "    Build the embedding matrix using pretrained word vectors\n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab: Fastai's Vocab object\n",
        "        see: https://docs.fast.ai/text.transform.html#Vocab\n",
        "    word_vectors_path:str\n",
        "        path to the pretrained word embeddings\n",
        "    verbose: Int. Default=1\n",
        "    Returns\n",
        "    -------\n",
        "    embedding_matrix: np.ndarray\n",
        "        pretrained word embeddings. If a word in our vocabulary is not among the\n",
        "        pretrained embeddings it will be assigned the mean pretrained\n",
        "        word-embeddings vector\n",
        "    \"\"\"\n",
        "    # if not os.path.isfile(word_vectors_path):\n",
        "    #     raise FileNotFoundError(\"{} not found\".format(word_vectors_path))\n",
        "    if verbose: print('Indexing word vectors...')\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(word_vectors_path)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    if verbose:\n",
        "        print('Loaded {} word vectors'.format(len(embeddings_index)))\n",
        "        print('Preparing embeddings matrix...')\n",
        "\n",
        "    mean_word_vector = np.mean(list(embeddings_index.values()), axis=0)\n",
        "    embedding_dim = len(list(embeddings_index.values())[0])\n",
        "    num_words = len(vocab.itos)\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "    found_words=0\n",
        "    for i,word in enumerate(vocab.itos):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            found_words+=1\n",
        "        else:\n",
        "            embedding_matrix[i] = mean_word_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KIXrzQtLy95J",
        "outputId": "77c933aa-20e1-4945-b66a-44420d8cf09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "embeddings_matrix = build_embeddings_matrix(vocab, path + 'glove.6B.300d.txt')\n",
        "embeddings_matrix.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors...\n",
            "Loaded 400000 word vectors\n",
            "Preparing embeddings matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FitRwC-BA3L9"
      },
      "source": [
        "## Model: HAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnggZJOd8mW1",
        "colab": {}
      },
      "source": [
        "class AttentionWithContext(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(AttentionWithContext, self).__init__()\n",
        "\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Calculate alignment scores\n",
        "        u = torch.tanh_(self.attn(inp))\n",
        "        # Using softmax to calculate attn_weights\n",
        "        a = F.softmax(self.contx(u), dim=1)\n",
        "        # Using dot product to calculate output\n",
        "        s = (a * inp).sum(1)\n",
        "        # Return normalized importance weights\n",
        "        return a.permute(0, 2, 1), s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sgm9EtmF87fn",
        "colab": {}
      },
      "source": [
        "class WordAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        hidden_dim=32,\n",
        "        padding_idx=1,\n",
        "        embed_dim=50,\n",
        "        embedding_matrix=None,\n",
        "        embed_drop=0.0,\n",
        "        weight_drop=0.0,\n",
        "    ):\n",
        "        super(WordAttnNet, self).__init__()\n",
        "\n",
        "        if isinstance(embedding_matrix, np.ndarray):\n",
        "            self.word_embed = nn.Embedding(\n",
        "                vocab_size, embedding_matrix.shape[1], padding_idx=padding_idx\n",
        "            )\n",
        "            self.word_embed.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
        "            embed_dim = embedding_matrix.shape[1]\n",
        "        else:\n",
        "            self.word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "\n",
        "        if embed_drop:\n",
        "            self.word_embed = EmbeddingDropout(self.word_embed, embed_drop)\n",
        "\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        if weight_drop:\n",
        "            self.rnn = WeightDropout(self.rnn, weight_drop)\n",
        "\n",
        "        self.word_attn = AttentionWithContext(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, X, h_n):\n",
        "        embed = self.word_embed(X.long())\n",
        "        h_t, h_n = self.rnn(embed, h_n)\n",
        "        a, s = self.word_attn(h_t)\n",
        "        return a, s.unsqueeze(1), h_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNMonhcS9Fos",
        "colab": {}
      },
      "source": [
        "class SentAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        word_hidden_dim=32, \n",
        "        sent_hidden_dim=32, \n",
        "        padding_idx=1,\n",
        "        weight_drop=0.0,\n",
        "    ):\n",
        "        super(SentAttnNet, self).__init__()\n",
        "\n",
        "        self.rnn = nn.GRU(\n",
        "            word_hidden_dim * 2, sent_hidden_dim, bidirectional=True, batch_first=True\n",
        "        )\n",
        "\n",
        "        if weight_drop:\n",
        "            self.rnn = WeightDropout(self.rnn, weight_drop)\n",
        "\n",
        "        self.sent_attn = AttentionWithContext(sent_hidden_dim * 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_t, h_n = self.rnn(X)\n",
        "        a, v = self.sent_attn(h_t)\n",
        "        return a.permute(0,2,1), v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOWveLmW9M2I",
        "colab": {}
      },
      "source": [
        "class HierAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        maxlen_sent,\n",
        "        maxlen_doc,\n",
        "        word_hidden_dim=32,\n",
        "        sent_hidden_dim=32,\n",
        "        padding_idx=1,\n",
        "        embed_dim=50,\n",
        "        weight_drop=0.0,\n",
        "        embed_drop=0.0,\n",
        "        last_drop=0.0,\n",
        "        embedding_matrix=None,\n",
        "        num_class=2,\n",
        "    ):\n",
        "        super(HierAttnNet, self).__init__()\n",
        "\n",
        "        self.word_hidden_dim = word_hidden_dim\n",
        "\n",
        "        self.wordattnnet = WordAttnNet(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_dim=word_hidden_dim,\n",
        "            padding_idx=padding_idx,\n",
        "            embed_dim=embed_dim,\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            embed_drop=embed_drop,\n",
        "            weight_drop=weight_drop,\n",
        "        )\n",
        "\n",
        "        self.sentattnnet = SentAttnNet(\n",
        "            word_hidden_dim=word_hidden_dim,\n",
        "            sent_hidden_dim=sent_hidden_dim,\n",
        "            padding_idx=padding_idx,\n",
        "            weight_drop=weight_drop,\n",
        "        )\n",
        "\n",
        "        self.ld = nn.Dropout(last_drop)\n",
        "        self.fc = nn.Linear(sent_hidden_dim * 2, num_class)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = X.permute(1, 0, 2)\n",
        "        word_h_n = torch.zeros((2,X.shape[0],self.word_hidden_dim))\n",
        "        if use_cuda:\n",
        "            word_h_n = word_h_n.cuda()\n",
        "        # alpha and s Tensor Lists\n",
        "        word_a_list, word_s_list = [], []\n",
        "        for sent in x:\n",
        "            word_a, word_s, word_h_n = self.wordattnnet(sent, word_h_n)\n",
        "            word_a_list.append(word_a)\n",
        "            word_s_list.append(word_s)\n",
        "        # Importance attention weights per word in sentence\n",
        "        self.sent_a = torch.cat(word_a_list, 1)\n",
        "        # Sentences representation\n",
        "        sent_s = torch.cat(word_s_list, 1)\n",
        "        # Importance attention weights per sentence in doc and document representation\n",
        "        self.doc_a, doc_s = self.sentattnnet(sent_s)\n",
        "        # Dropout\n",
        "        doc_s = self.ld(doc_s)\n",
        "        return self.fc(doc_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GaRZzoZmk7za",
        "outputId": "7fb66b29-c218-4882-b9cf-bf9303d65978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "m = HierAttnNet(\n",
        "    vocab_size=len(vocab.stoi),\n",
        "    maxlen_sent=maxlen_sent,\n",
        "    maxlen_doc=maxlen_doc,\n",
        "    word_hidden_dim=32,\n",
        "    sent_hidden_dim=32,\n",
        "    padding_idx=1,\n",
        "    embed_dim=300,\n",
        "    weight_drop=0.8,\n",
        "    embed_drop=0.8,\n",
        "    last_drop=0.8,\n",
        "    embedding_matrix=embeddings_matrix,\n",
        "    num_class=2,\n",
        ")\n",
        "\n",
        "if use_cuda:\n",
        "    m = m.cuda()\n",
        "\n",
        "m"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HierAttnNet(\n",
              "  (wordattnnet): WordAttnNet(\n",
              "    (word_embed): EmbeddingDropout(\n",
              "      (emb): Embedding(74813, 300, padding_idx=1)\n",
              "    )\n",
              "    (rnn): WeightDropout(\n",
              "      (module): GRU(300, 32, batch_first=True, bidirectional=True)\n",
              "    )\n",
              "    (word_attn): AttentionWithContext(\n",
              "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (sentattnnet): SentAttnNet(\n",
              "    (rnn): WeightDropout(\n",
              "      (module): GRU(64, 32, batch_first=True, bidirectional=True)\n",
              "    )\n",
              "    (sent_attn): AttentionWithContext(\n",
              "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (ld): Dropout(p=0.8, inplace=False)\n",
              "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m2fw4jTgLU5v"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fhGfU8hLLrPi"
      },
      "source": [
        "Definition of training loop, where we return the model at the lowest validation loss during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uKRzwb6ELWpo",
        "colab": {}
      },
      "source": [
        "def fit(model, train_dl, val_dl, loss_fn, opt, epochs, batch_size):\n",
        "    measurements = {\n",
        "        \"Train Loss\": [],\n",
        "        \"Train Acc\": [],\n",
        "        \"Train F1\": [],\n",
        "        \"Val Loss\": [],\n",
        "        \"Val Acc\": [],\n",
        "        \"Val F1\": [],\n",
        "        \"Lowest Val Loss\": np.Inf,\n",
        "        \"Lowest Val Loss Epoch\": np.Inf\n",
        "    }\n",
        "    lowest_val_loss = np.Inf\n",
        "    lowest_val_loss_epoch = np.Inf\n",
        "    \n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training\n",
        "        y_true_train = list()\n",
        "        y_pred_train = list()\n",
        "        total_loss_train = 0  \n",
        "        \n",
        "        model.train() # Set model in train mode\n",
        "        \n",
        "        for X,y in iter(train_dl):\n",
        "            X = X.cuda() if use_cuda else X\n",
        "            y = y.cuda() if use_cuda else y\n",
        "\n",
        "            opt.zero_grad() # Clear old gradients\n",
        "            pred = model(X) # Make prediction using model\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward() # Compute derivatives of loss w.r.t. parameters using backpropagation\n",
        "            opt.step() # Take a step using computed gradients\n",
        "            pred_idx = torch.max(pred, 1)[1]\n",
        "            \n",
        "            y_true_train += list(y.cpu().data.numpy())\n",
        "            y_pred_train += list(pred_idx.cpu().data.numpy())\n",
        "            total_loss_train += loss.item()\n",
        "        \n",
        "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
        "        train_loss = total_loss_train/len(train_dl)\n",
        "        measurements[\"Train Acc\"].append(train_acc)\n",
        "        measurements[\"Train F1\"].append(train_f1)\n",
        "        measurements[\"Train Loss\"].append(train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        y_true_val = list()\n",
        "        y_pred_val = list()\n",
        "        total_loss_val = 0\n",
        "        \n",
        "        model.eval() # Set model in eval mode\n",
        "\n",
        "        for X,y in iter(val_dl):\n",
        "            X = X.cuda() if use_cuda else X\n",
        "            y = y.cuda() if use_cuda else y\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            pred_idx = torch.max(pred, 1)[1]\n",
        "\n",
        "            y_true_val += list(y.cpu().data.numpy())\n",
        "            y_pred_val += list(pred_idx.cpu().data.numpy())\n",
        "            total_loss_val += loss.item()\n",
        "\n",
        "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
        "        val_f1 = f1_score(y_true_val, y_pred_val)\n",
        "        val_loss = total_loss_val/len(val_dl)\n",
        "        measurements[\"Val Acc\"].append(val_acc)\n",
        "        measurements[\"Val F1\"].append(val_f1)\n",
        "        measurements[\"Val Loss\"].append(val_loss)\n",
        "        \n",
        "        if (val_loss < measurements[\"Lowest Val Loss\"]):\n",
        "            torch.save(model.state_dict(), path + model_name + '_checkpoint.pt')\n",
        "            measurements[\"Lowest Val Loss\"] = val_loss\n",
        "            measurements[\"Lowest Val Loss Epoch\"] = epoch\n",
        "        \n",
        "        print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} train_f1: {train_f1:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} val_f1: {val_f1:.4f}')\n",
        "        \n",
        "    # Return model checkpoint stored at lowest validation loss\n",
        "    model.load_state_dict(torch.load(path + model_name + '_checkpoint.pt'))\n",
        "    print(f'Lowest val_loss: {measurements[\"Lowest Val Loss\"]:.4f}, at epoch {measurements[\"Lowest Val Loss Epoch\"]}')\n",
        "    \n",
        "    return model, measurements"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b898edf-9cc1-4287-b829-eff4fff7e8d3",
        "id": "xnqpxiOEmeBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "opt = torch.optim.AdamW(m.parameters())\n",
        "m, measurements = fit(\n",
        "    model=m,\n",
        "    train_dl=train_loader, \n",
        "    val_dl=val_loader, \n",
        "    loss_fn=F.cross_entropy,\n",
        "    opt=opt,\n",
        "    epochs=20,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss: 0.4727 train_acc: 0.7860 train_f1: 0.7905 | val_loss: 0.4460 val_acc: 0.7931 val_f1: 0.8068\n",
            "Epoch 2: train_loss: 0.4526 train_acc: 0.7952 train_f1: 0.8009 | val_loss: 0.4313 val_acc: 0.8000 val_f1: 0.8128\n",
            "Epoch 3: train_loss: 0.4488 train_acc: 0.7988 train_f1: 0.8047 | val_loss: 0.4303 val_acc: 0.8020 val_f1: 0.8180\n",
            "Epoch 4: train_loss: 0.4445 train_acc: 0.7990 train_f1: 0.8049 | val_loss: 0.4200 val_acc: 0.8043 val_f1: 0.8152\n",
            "Epoch 5: train_loss: 0.4440 train_acc: 0.7981 train_f1: 0.8045 | val_loss: 0.4165 val_acc: 0.8057 val_f1: 0.8131\n",
            "Epoch 6: train_loss: 0.4409 train_acc: 0.8018 train_f1: 0.8077 | val_loss: 0.4170 val_acc: 0.8071 val_f1: 0.8174\n",
            "Epoch 7: train_loss: 0.4381 train_acc: 0.8016 train_f1: 0.8076 | val_loss: 0.4182 val_acc: 0.8077 val_f1: 0.8163\n",
            "Epoch 8: train_loss: 0.4392 train_acc: 0.8004 train_f1: 0.8069 | val_loss: 0.4161 val_acc: 0.8056 val_f1: 0.8156\n",
            "Epoch 9: train_loss: 0.4369 train_acc: 0.8029 train_f1: 0.8090 | val_loss: 0.4154 val_acc: 0.8106 val_f1: 0.8184\n",
            "Epoch 10: train_loss: 0.4365 train_acc: 0.8021 train_f1: 0.8080 | val_loss: 0.4163 val_acc: 0.8099 val_f1: 0.8097\n",
            "Epoch 11: train_loss: 0.4363 train_acc: 0.8017 train_f1: 0.8077 | val_loss: 0.4161 val_acc: 0.8108 val_f1: 0.8138\n",
            "Epoch 12: train_loss: 0.4335 train_acc: 0.8031 train_f1: 0.8092 | val_loss: 0.4169 val_acc: 0.8089 val_f1: 0.8192\n",
            "Epoch 13: train_loss: 0.4341 train_acc: 0.8026 train_f1: 0.8088 | val_loss: 0.4202 val_acc: 0.8063 val_f1: 0.8026\n",
            "Epoch 14: train_loss: 0.4328 train_acc: 0.8035 train_f1: 0.8094 | val_loss: 0.4144 val_acc: 0.8089 val_f1: 0.8115\n",
            "Epoch 15: train_loss: 0.4330 train_acc: 0.8037 train_f1: 0.8094 | val_loss: 0.4114 val_acc: 0.8128 val_f1: 0.8177\n",
            "Epoch 16: train_loss: 0.4316 train_acc: 0.8044 train_f1: 0.8099 | val_loss: 0.4108 val_acc: 0.8142 val_f1: 0.8177\n",
            "Epoch 17: train_loss: 0.4305 train_acc: 0.8042 train_f1: 0.8101 | val_loss: 0.4138 val_acc: 0.8105 val_f1: 0.8095\n",
            "Epoch 18: train_loss: 0.4312 train_acc: 0.8042 train_f1: 0.8096 | val_loss: 0.4105 val_acc: 0.8093 val_f1: 0.8116\n",
            "Epoch 19: train_loss: 0.4309 train_acc: 0.8055 train_f1: 0.8112 | val_loss: 0.4142 val_acc: 0.8096 val_f1: 0.8092\n",
            "Epoch 20: train_loss: 0.4299 train_acc: 0.8052 train_f1: 0.8110 | val_loss: 0.4120 val_acc: 0.8097 val_f1: 0.8090\n",
            "Lowest val_loss: 0.4105, at epoch 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "518a15dc-2f3c-4b16-8f78-2901a604bac4",
        "id": "M7Nol7wpmeBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,21), measurements[\"Train Loss\"], label=\"Train Loss\")\n",
        "plt.plot(range(1,21), measurements[\"Val Loss\"], label=\"Validation Loss\")\n",
        "plt.axvline(measurements[\"Lowest Val Loss Epoch\"], linestyle='--', color='r',label='Checkpoint')\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f28e42226d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z_6KnEeb04_r"
      },
      "source": [
        "## Test Accuracy and Attention Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MObfLOhKqUBp",
        "colab": {}
      },
      "source": [
        "def get_predictions_and_attn_weights(model, test_loader):\n",
        "    model.eval()\n",
        "    preds, sent_a, doc_a = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            X = data.cuda() if use_cuda else data\n",
        "            y_pred = model(X)\n",
        "            preds.append(y_pred)\n",
        "            sent_a.append(model.sent_a)\n",
        "            doc_a.append(model.doc_a)\n",
        "    return preds, sent_a, doc_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Sk8iacXHvwx",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "def plot_attention(doc, word_w, doc_w):\n",
        "    word_cmap = matplotlib.cm.get_cmap(\"Greens\")\n",
        "    sent_cmap = matplotlib.cm.get_cmap(\"Reds\")\n",
        "    template = '<font face=\"monospace\" \\nsize=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
        "    colored_doc = \"\"\n",
        "    for sent, sent_w, whole_sent_w in zip(doc, word_w, doc_w):\n",
        "        sent_len, pad_count = len(sent.split()), 0\n",
        "        sentence_fmt = \"\"\n",
        "        sent_color = matplotlib.colors.rgb2hex(sent_cmap(whole_sent_w)[:3])\n",
        "        \n",
        "        for t, w in zip(sent.split(), sent_w):\n",
        "            if t == 'xxpad':\n",
        "                pad_count+=1\n",
        "                continue\n",
        "            word_color = matplotlib.colors.rgb2hex(word_cmap(w)[:3])\n",
        "            sentence_fmt += template.format(word_color, \"&nbsp\" + t +\"&nbsp\")\n",
        "        if pad_count != sent_len:\n",
        "            sentence_fmt += \"</br>\"\n",
        "            sentence_fmt = template.format(sent_color, \"&nbsp&nbsp&nbsp\") + sentence_fmt\n",
        "        colored_doc += sentence_fmt\n",
        "        \n",
        "    return colored_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTd_hyTT1JhI",
        "colab": {}
      },
      "source": [
        "preds_l, word_attn_l, sent_attn_l = get_predictions_and_attn_weights(m, test_loader)\n",
        "\n",
        "preds = F.softmax(torch.cat(preds_l), 1).cpu().numpy() # Probabilities of being not useful & useful [0,1] per example\n",
        "word_attn = torch.cat(word_attn_l).cpu().numpy()\n",
        "sent_attn = torch.cat(sent_attn_l).squeeze(2).cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5VWsHhVG_sF",
        "outputId": "b1d04e2a-c86e-4603-beb3-6eb571e417ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y_true = test[target].to_numpy()\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print('Test Accuracy: {}'.format(accuracy_score(y_true, y_pred)))\n",
        "print('Test F1: {}'.format(f1_score(y_true, y_pred)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8121\n",
            "Test F1: 0.8160548213411649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uN6v4-CC1SfT",
        "colab": {}
      },
      "source": [
        "def get_indices(y_true, y_preds, n=1, agree=True, is_pos=True):\n",
        "    pred_pos = np.where(y_preds[:, 1] > 0.9)[0]\n",
        "    true_pos = np.where(y_true == 1)[0]\n",
        "    pred_neg = np.where(y_preds[:, 0] > 0.9)[0] \n",
        "    true_neg = np.where(y_true == 0)[0]\n",
        "    # if prediction and real agree\n",
        "    if agree:\n",
        "        # if real is positive\n",
        "        if is_pos:\n",
        "            idx = np.random.choice(np.intersect1d(pred_pos, true_pos), n)[0]\n",
        "        else: \n",
        "            idx = np.random.choice(np.intersect1d(pred_neg, true_neg), n)[0]\n",
        "    else:\n",
        "        if is_pos:\n",
        "            idx = np.random.choice(np.intersect1d(pred_neg, true_pos), n)[0]  \n",
        "        else: \n",
        "            idx = np.random.choice(np.intersect1d(pred_pos, true_neg), n)[0]  \n",
        "    return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUR_Pkci1jAs",
        "colab": {}
      },
      "source": [
        "idx = get_indices(y_true, preds)\n",
        "idx = 999\n",
        "\n",
        "doc = [vocab.textify(s) for s in test_padded_texts[idx]]\n",
        "word_w = word_attn[idx]\n",
        "doc_w  = sent_attn[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QMg2wCM7f5j",
        "colab": {}
      },
      "source": [
        "pred_pos_real_pos_attn = plot_attention(doc, word_w, doc_w)\n",
        "with open('/tmp/pred_pos_real_pos_attn.html', 'w') as f:\n",
        "    f.write(pred_pos_real_pos_attn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1er3STW7igY",
        "outputId": "02e3d709-7586-4ce9-9765-5af21bae7af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML('/tmp/pred_pos_real_pos_attn.html'))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fdd5c4\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ebf7e7\">&nbspgot&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspthis&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e9f7e5\">&nbsptelescope&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspa&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #dff3da\">&nbspfew&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspdays&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspbefore&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspchristmas&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspfor&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ed\">&nbspmy&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspyear&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspold&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbsps&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspbig&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e8f6e3\">&nbspgift&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspof&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspthe&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspyear&nbsp</span></br><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fdd0bc\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspit&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspworked&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspwhen&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspi&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspset&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspit&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspup&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspbut&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspthe&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e8f6e4\">&nbspcomputer&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspdied&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ed\">&nbspcompletely&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspon&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspchristmas&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #edf8e9\">&nbspnight&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspwhen&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eff9ec\">&nbspwe&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspfirst&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspwent&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eef8ea\">&nbspout&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspto&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspuse&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspit&nbsp</span></br><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fedaca\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspit&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ebf7e7\">&nbspwas&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspa&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspmajor&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e8f6e3\">&nbspbummer&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspand&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspreally&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e9f7e5\">&nbspsucked&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eff9ec\">&nbspto&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e4f5df\">&nbspkill&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e9f7e5\">&nbsphis&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e9f7e5\">&nbspchristmas&nbsp</span></br><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fcb499\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspwas&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf2\">&nbspto&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspbuy&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspa&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ed\">&nbspserial&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ecf8e8\">&nbspadapter&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e9f7e5\">&nbspcable&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspfrom&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #e8f6e4\">&nbspthem&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f5fbf2\">&nbspand&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspupdate&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspthe&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #edf8e9\">&nbspcomputer&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf2\">&nbsps&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspfirmware&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspsince&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf2\">&nbspit&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspdoesn&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspt&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbsptake&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f5fbf2\">&nbspa&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f4fbf1\">&nbspnormal&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspserial&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eff9eb\">&nbspcable&nbsp</span></br><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fed8c7\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eff9ec\">&nbspi&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspm&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspa&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faf0\">&nbspxxunk&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f2faef\">&nbspand&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f0f9ec\">&nbspi&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ecf8e8\">&nbspeven&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f3faf0\">&nbspattempted&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspto&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspmake&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #edf8ea\">&nbspmy&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #eff9eb\">&nbspown&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspbut&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #f1faee\">&nbspit&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ebf7e7\">&nbspwas&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ecf8e8\">&nbspno&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #ecf8e8\">&nbspgood&nbsp</span></br><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #fff2ec\">&nbsp&nbsp&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #c9eac2\">&nbspmajor&nbsp</span><font face=\"monospace\" \n",
              "size=\"3\"; span class=\"barcode\"; style=\"color: black; background-color: #81ca81\">&nbspbummer&nbsp</span></br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tN_3KePCgIK4"
      },
      "source": [
        "## Top word & sentence attention weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-olt-ap7YLz",
        "outputId": "19197830-e21c-4d1a-c352-4536bbdd6569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Top 10 average word attention weights\n",
        "\n",
        "avg_weights = {}\n",
        "for i, review in enumerate(test_padded_texts):\n",
        "  word_weights = word_attn[i]\n",
        "  doc = [vocab.textify(s) for s in test_padded_texts[i]]\n",
        "\n",
        "  for j in range(word_weights.shape[0]):\n",
        "    words_in_sentence = doc[j].split()\n",
        "    for k in range(word_weights.shape[1]):\n",
        "      if len(words_in_sentence) < word_weights.shape[1]:\n",
        "        continue\n",
        "      word = words_in_sentence[k]\n",
        "      if word == 'xxpad':\n",
        "        continue\n",
        "      if word in avg_weights:\n",
        "        avg_weights[word] = (avg_weights[word][0] + word_weights[j][k], avg_weights[word][1] + 1)\n",
        "      else:\n",
        "        avg_weights[word] = (word_weights[j][k], 1)\n",
        "\n",
        "for k,v in avg_weights.items():\n",
        "  avg_weights[k] = v[0] / v[1]\n",
        "\n",
        "sorted(avg_weights.items(), key=lambda item: item[1], reverse=True)[:10]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hmm', 0.7288912534713745),\n",
              " ('skinny', 0.673136830329895),\n",
              " ('discwasher', 0.6393969655036926),\n",
              " ('overrated', 0.5942797660827637),\n",
              " ('positives', 0.5590387344360351),\n",
              " ('lol', 0.5450962066650391),\n",
              " ('snowflake', 0.5320731401443481),\n",
              " ('bravo', 0.5173042416572571),\n",
              " ('excelente', 0.5091871193477085),\n",
              " ('skyfi', 0.5024579763412476)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-MAbFODbsani",
        "outputId": "83d098cb-ed57-48f7-fb69-e917ec97d9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Top 10 sentence attention weights\n",
        "\n",
        "sent_weights = {}\n",
        "\n",
        "for i, review in enumerate(test_padded_texts):\n",
        "    sent_atten_per_doc = sent_attn[i]\n",
        "    doc = [vocab.textify(s) for s in test_padded_texts[i]]\n",
        "\n",
        "    for j in range(len(sent_atten_per_doc)):\n",
        "        words_in_sentence = doc[j].split()\n",
        "        words_in_sentence = list(filter(lambda a: a != 'xxpad', words_in_sentence))\n",
        "        original_sentence = \" \".join(x for x in words_in_sentence)\n",
        "        sent_weights[original_sentence] = sent_atten_per_doc[j]\n",
        "\n",
        "sorted(sent_weights.items(), key=lambda item: item[1], reverse=True)[:10]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('type keyboards no trouble using the keyboard with any computer with a ps connector but the usb converter is not xxunk with the dell',\n",
              "  0.7448734),\n",
              " ('wallet but looks ok and performs ok i rarely have to point it anywhere i m in north xxunk in order to get reception',\n",
              "  0.74380964),\n",
              " ('and one cable to the y splitter the signal will bounce in the cable and produce really ugly ghosting of edges in the image',\n",
              "  0.7385943),\n",
              " ('makes it easy to hall my gear around town xxunk on my f and fx but xxunk not fit the f bag and strap',\n",
              "  0.73735064),\n",
              " ('and smells good in your car or truck has good dimensions for the that i bought any questions email me at xxunk yahoo com',\n",
              "  0.7371715),\n",
              " ('lens will not zoom in and out as it should the money that paid for this lens i would like to have it working',\n",
              "  0.7364107),\n",
              " ('of space between lcd plasma screen and the wall as it will stick out inch and you still need inch for the power plug',\n",
              "  0.7361024),\n",
              " ('work right out of the box with ubuntu no need to change anything just run indicating the number of device and that s all',\n",
              "  0.7359495),\n",
              " ('that the same manufacturer s drive and media would be compatible but in this crazy mixed up technology world i am not so sure',\n",
              "  0.73561513),\n",
              " ('overall protection of your lenses but not multi coated glass so pretty significant problem with ghosting and lens flare when shooting towards a light',\n",
              "  0.73557144)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mDTJ3NRp3yoL"
      },
      "source": [
        "##Â Word Level Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e51VnvXh94XF",
        "outputId": "096b53c1-4f02-490e-f98a-cead91e1f91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "count=500"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NC_kpAVT_IuN",
        "outputId": "21a939ac-3977-4a48-af62-f9e5f3b93c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Distribution of POS tags of top attended words using NLTK\n",
        "top_attended_words = sorted(avg_weights.items(), key=lambda item: item[1], reverse=True)[:count]\n",
        "top_attended_words = [w[0] for w in top_attended_words]\n",
        "# print(top_attended_words)\n",
        "top_pos_tags = [w[1] for w in nltk.pos_tag(top_attended_words, tagset='universal')]\n",
        "df = pd.DataFrame(top_pos_tags)\n",
        "df[0].value_counts()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOUN    265\n",
              "ADJ      99\n",
              "VERB     97\n",
              "ADV      32\n",
              "ADP       3\n",
              "X         3\n",
              "DET       1\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd7s3Og78Ds9",
        "outputId": "e270da4c-6576-46a6-bf6f-669a6ee65f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Distribution of POS tags of bottom attended words using NLTK\n",
        "bottom_attended_words = sorted(avg_weights.items(), key=lambda item: item[1], reverse=False)[:count]\n",
        "bottom_attended_words = [w[0] for w in bottom_attended_words]\n",
        "# print(bottom_attended_words)\n",
        "bottom_pos_tags = [w[1] for w in nltk.pos_tag(bottom_attended_words, tagset='universal')]\n",
        "df = pd.DataFrame(bottom_pos_tags)\n",
        "df[0].value_counts()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOUN    207\n",
              "VERB    156\n",
              "ADJ     101\n",
              "ADV      29\n",
              "ADP       3\n",
              "PRON      2\n",
              "NUM       1\n",
              "PRT       1\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zt8WqMWU_056",
        "outputId": "2ecb5e86-dea6-4753-b314-3bae1e86c8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Distribution of POS tags of all words using NLTK\n",
        "all_words = [w[0] for w in avg_weights.items()]\n",
        "pos_tags = [w[1] for w in nltk.pos_tag(all_words, tagset='universal')]\n",
        "df = pd.DataFrame(pos_tags)\n",
        "df[0].value_counts()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOUN    4583\n",
              "VERB    2372\n",
              "ADJ     2046\n",
              "ADV      567\n",
              "ADP      108\n",
              "PRON      29\n",
              "DET       21\n",
              "NUM       18\n",
              "X         13\n",
              "PRT       12\n",
              "CONJ      10\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ybo64BrZ_KjO",
        "outputId": "6b8402d6-d0b0-420b-b0d8-e35acaa704d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "state_verbs = ['adore','agree','appear','appreciate','be','believe','belong','concern','consist','contain','cost','deny','depend','deserve','detest','disagree','dislike','doubt','equal','feel','hate','have','hear','imagine','include','involve','know','lack','like','loathe','look','love','matter','mean','measure','mind','need','owe','own','possess','promise','realize','recognize','remember','resemble','satisfy','see','seem','smell','sound','suppose','surprise','taste','think','understand','want','weigh','wish']\n",
        "\n",
        "lemmatized_top_attended_words = [lemmatizer.lemmatize(i) for i in top_attended_words]\n",
        "lemmatized_bottom_attended_words = [lemmatizer.lemmatize(i) for i in bottom_attended_words]\n",
        "lemmatized_all_words = [lemmatizer.lemmatize(i) for i in all_words]\n",
        "print([i for i in lemmatized_top_attended_words if i in state_verbs])\n",
        "print([i for i in lemmatized_bottom_attended_words if i in state_verbs])\n",
        "print([i for i in lemmatized_all_words if i in state_verbs])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['deserve', 'smell']\n",
            "['belong', 'wish', 'doubt']\n",
            "['have', 'be', 'see', 'mean', 'think', 'sound', 'seem', 'cost', 'matter', 'surprise', 'include', 'imagine', 'look', 'want', 'need', 'love', 'like', 'lack', 'need', 'feel', 'own', 'feel', 'mind', 'lack', 'look', 'mean', 'believe', 'hate', 'know', 'want', 'hear', 'measure', 'cost', 'disagree', 'know', 'sound', 'love', 'depend', 'agree', 'wish', 'remember', 'surprise', 'see', 'concern', 'taste', 'recognize', 'doubt', 'understand', 'concern', 'think', 'suppose', 'realize', 'like', 'taste', 'equal', 'appear', 'matter', 'appreciate', 'measure', 'weigh', 'contain', 'satisfy', 'equal', 'owe', 'deserve', 'dislike', 'promise', 'belong', 'wish', 'smell', 'dislike', 'promise', 'smell', 'doubt', 'involve']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrrPnGne9UxF",
        "outputId": "8d359747-3502-4e1a-a437-8edb92890489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('punkt')\n",
        "k=3"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "64DCZ2DHDN8P",
        "outputId": "840f5c9b-0917-4ac0-d0f1-ef86258f26e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Polarity/Subjectivity scores of all words using TextBlob\n",
        "\n",
        "top_word_pola_scores = []\n",
        "top_word_subj_scores = []\n",
        "bottom_word_pola_scores = []\n",
        "bottom_word_subj_scores = []\n",
        "all_word_pola_scores = []\n",
        "all_word_subj_scores = []\n",
        "\n",
        "for sent in top_attended_words:\n",
        "  s = TextBlob(sent)\n",
        "  top_word_pola_scores.append(s.sentiment.polarity)\n",
        "  top_word_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "for sent in bottom_attended_words:\n",
        "  s = TextBlob(sent)\n",
        "  bottom_word_pola_scores.append(s.sentiment.polarity)\n",
        "  bottom_word_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "for sent in all_words:\n",
        "  s = TextBlob(sent)\n",
        "  all_word_pola_scores.append(s.sentiment.polarity)\n",
        "  all_word_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "print('Top Polarity Mean: {}, Std: {}'.format(np.mean(top_word_pola_scores), np.std(top_word_pola_scores)))\n",
        "print('Top Subjectivity Mean: {}, Std: {}'.format(np.mean(top_word_subj_scores), np.std(top_word_subj_scores)))\n",
        "print('Bottom Polarity Mean: {}, Std: {}'.format(np.mean(bottom_word_pola_scores), np.std(bottom_word_pola_scores)))\n",
        "print('Bottom Subjectivity Mean: {}, Std: {}'.format(np.mean(bottom_word_subj_scores), np.std(bottom_word_subj_scores)))\n",
        "print('All Polarity Mean: {}, Std: {}'.format(np.mean(all_word_pola_scores), np.std(all_word_pola_scores)))\n",
        "print('All Subjectivity Mean: {}, Std: {}'.format(np.mean(all_word_subj_scores), np.std(all_word_subj_scores)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top Polarity Mean: -0.00275, Std: 0.1216087476294366\n",
            "Top Subjectivity Mean: 0.0346, Std: 0.16909417494402346\n",
            "Bottom Polarity Mean: 0.0065393939393939395, Std: 0.09329751555936551\n",
            "Bottom Subjectivity Mean: 0.03250909090909091, Std: 0.15240204115680447\n",
            "All Polarity Mean: 0.0055830209887373205, Std: 0.12886528184288576\n",
            "All Subjectivity Mean: 0.055258490103821944, Std: 0.19763752234526702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tg9qNxPP3yok"
      },
      "source": [
        "##Â Sentence Level Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HV0FtWqoHFTY",
        "outputId": "f2a50f79-2027-41a0-8e72-4a8578d13f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Automated Readability Index\n",
        "def ari(sentence):\n",
        "    chars = len(sentence.replace(' ', ''))\n",
        "    words = len(sentence.split(' '))\n",
        "    return 4.71 * chars / words + 0.5 * words - 21.43\n",
        "\n",
        "top_sentences = sorted(sent_weights.items(), key=lambda item: item[1], reverse=True)[:count]\n",
        "top_sentences = [s[0] for s in top_sentences]\n",
        "\n",
        "top_ari = [ari(s) for s in top_sentences]\n",
        "print('Top Mean: {}, Std: {}'.format(np.mean(top_ari), np.std(top_ari)))\n",
        "\n",
        "bottom_sentences = sorted(sent_weights.items(), key=lambda item: item[1], reverse=False)[:count]\n",
        "bottom_sentences = [s[0] for s in bottom_sentences]\n",
        "\n",
        "bottom_ari = [ari(s) for s in bottom_sentences]\n",
        "print('Bottom Mean: {}, Std: {}'.format(np.mean(bottom_ari), np.std(bottom_ari)))\n",
        "\n",
        "all_ari = [ari(s[0]) for s in sent_weights.items()]\n",
        "print('All Mean: {}, Std: {}'.format(np.mean(all_ari), np.std(all_ari)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top Mean: 8.314571055649045, Std: 2.810831404651313\n",
            "Bottom Mean: 2.858531696553447, Std: 6.39919667758775\n",
            "All Mean: 5.834034933356928, Std: 4.443445941699827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPoip51BI74Y",
        "colab": {}
      },
      "source": [
        "top_attended_sents = sorted(sent_weights.items(), key=lambda item: item[1], reverse=True)[:100]\n",
        "top_attended_sents = [w[0] for w in top_attended_sents]\n",
        "\n",
        "bottom_attended_sents = sorted(sent_weights.items(), key=lambda item: item[1], reverse=False)[:100]\n",
        "bottom_attended_sents = [w[0] for w in bottom_attended_sents]\n",
        "\n",
        "all_attended_sents = [w[0] for w in sent_weights.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ra6C0iqe3yon",
        "outputId": "b220c939-e0a9-4046-8c9b-178334c3c2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Sentence Polarity & Subjectivity using TextBlob\n",
        "from textblob import TextBlob\n",
        "top_sent_pola_scores = []\n",
        "top_sent_subj_scores = []\n",
        "bottom_sent_pola_scores = []\n",
        "bottom_sent_subj_scores = []\n",
        "all_sent_pola_scores = []\n",
        "all_sent_subj_scores = []\n",
        "\n",
        "for sent in top_attended_sents:\n",
        "  s = TextBlob(sent)\n",
        "  top_sent_pola_scores.append(s.sentiment.polarity)\n",
        "  top_sent_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "for sent in bottom_attended_sents:\n",
        "  s = TextBlob(sent)\n",
        "  bottom_sent_pola_scores.append(s.sentiment.polarity)\n",
        "  bottom_sent_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "for sent in all_attended_sents:\n",
        "  s = TextBlob(sent)\n",
        "  all_sent_pola_scores.append(s.sentiment.polarity)\n",
        "  all_sent_subj_scores.append(s.sentiment.subjectivity)\n",
        "\n",
        "print('Top Polarity Mean: {}, Std: {}'.format(np.mean(top_sent_pola_scores), np.std(top_sent_pola_scores)))\n",
        "print('Top Subjectivity Mean: {}, Std: {}'.format(np.mean(top_sent_subj_scores), np.std(top_sent_subj_scores)))\n",
        "print('Bottom Polarity Mean: {}, Std: {}'.format(np.mean(bottom_sent_pola_scores), np.std(bottom_sent_pola_scores)))\n",
        "print('Bottom Subjectivity Mean: {}, Std: {}'.format(np.mean(bottom_sent_subj_scores), np.std(bottom_sent_subj_scores)))\n",
        "print('All Polarity Mean: {}, Std: {}'.format(np.mean(all_sent_pola_scores), np.std(all_sent_pola_scores)))\n",
        "print('All Subjectivity Mean: {}, Std: {}'.format(np.mean(all_sent_subj_scores), np.std(all_sent_subj_scores)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top Polarity Mean: 0.18950417619417617, Std: 0.29150043610142906\n",
            "Top Subjectivity Mean: 0.44302264716764717, Std: 0.2774646488537869\n",
            "Bottom Polarity Mean: 0.24083571428571432, Std: 0.3432976497951527\n",
            "Bottom Subjectivity Mean: 0.380002380952381, Std: 0.3752056303109094\n",
            "All Polarity Mean: 0.14996197400959196, Std: 0.29380648384717645\n",
            "All Subjectivity Mean: 0.4007260598499362, Std: 0.306813019538775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njrlwQCk_Rm1",
        "colab_type": "text"
      },
      "source": [
        "##Â References\n",
        "\n",
        "https://towardsdatascience.com/predicting-amazon-reviews-scores-using-hierarchical-attention-networks-with-pytorch-and-apache-5214edb3df20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky1VhDuAA7vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}